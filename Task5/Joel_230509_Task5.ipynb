{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8833456,"sourceType":"datasetVersion","datasetId":5315433},{"sourceId":8833460,"sourceType":"datasetVersion","datasetId":5315436}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nc = 0;\nfor dirname, _, filenames in os.walk('/kaggle/input/noised-dataset'):\n    for filename in filenames:\n        path = (os.path.join(dirname, filename))\n        c+=1\nc\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Have to use Wave-UNET**","metadata":{}},{"cell_type":"code","source":"clean = '/kaggle/input/cleaned-dataset'\nnoise = '/kaggle/input/noised-dataset'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time;\nst = time.time();\nfile_path = '/kaggle/input/clean-audio/Clean/03-01-01-01-01-01-03.wav'\naudio,sr = librosa.load(file_path)\nprint(time.time()-st)\nduration = librosa.get_duration(y=audio, sr=sr)\nprint(duration,sr)\n22050*3.55","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_list = []\nnoise_list = []\nduration = 2.972154\nsr = 22050\nfor dirname, _, filenames in os.walk('/kaggle/input/clean-audio'):\n    for filename in filenames:\n        path_clean = os.path.join(dirname, filename)\n        audio,_ = librosa.load(path_clean, sr=sr, duration=duration)\n        clean_list.append(audio)\n        \n        filename = filename[:len(filename)-4]\n        filename += '_noisy.wav'\n        path_noisy = os.path.join(noise,filename)\n        audio,_ = librosa.load(path_noisy, sr=sr, duration=duration)\n        noise_list.append(audio)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(noise_list[0]),len(clean_list))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise_list = np.array(noise_list)\nclean_list = np.array(clean_list)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the U-NET MODEL","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndef conv_block(inputs, num_filters):\n    x = tf.keras.layers.Conv2D(num_filters, 3, padding=\"same\")(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.01)(x)\n\n    x = tf.keras.layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.01)(x)\n\n    return x\n\ndef encoder_block(inputs, num_filters):\n    x = conv_block(inputs, num_filters)\n    p = tf.keras.layers.MaxPool2D((2,2))(x)\n    return x, p\n\ndef decoder_block(inputs, skip, num_filters):\n    x = tf.keras.layers.Conv2DTranspose(num_filters, (2,2), strides=2, padding=\"same\")(inputs)\n    x = tf.keras.layers.Concatenate()([x, skip])\n    x = conv_block(x, num_filters)\n    return x\n\ndef build_unet(input_shape):\n    inputs = tf.keras.layers.Input(input_shape)\n\n    # Encoder\n    s1, p1 = encoder_block(inputs, 64)   # 256 x 256\n    s2, p2 = encoder_block(p1, 128)     # 128 x 128\n    s3, p3 = encoder_block(p2, 256)     # 64 x 64\n     # 32 x 32\n\n    # Bridge\n    b1 = conv_block(p3, 1024)           # 16 x 16\n\n    # Decoder\n#     d1 = decoder_block(b1, s4, 512)     # 32 x 32\n    d2 = decoder_block(b1, s3, 256)     # 64 x 64\n    d3 = decoder_block(d2, s2, 128)     # 128 x 128\n    d4 = decoder_block(d3, s1, 64)      # 256 x 256\n\n    outputs = tf.keras.layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n    model = tf.keras.models.Model(inputs, outputs, name=\"UNET\")\n    return model\n\ninput_shape = (64, 128, 1)\nmodel = build_unet(input_shape)\nmodel.summary()\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nXnoisy_train, Xnoisy_val, Yclean_train, Yclean_val = train_test_split(noise_list, clean_list, test_size=0.2, random_state=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xnoisy_train = Xnoisy_train[..., np.newaxis]\nYclean_train = Yclean_train[..., np.newaxis]\nXnoisy_val = Xnoisy_val[..., np.newaxis]\nYclean_val = Yclean_val[..., np.newaxis]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\nepochs = 20\n\nhistory = model.fit(Xnoisy_train, Yclean_train, validation_data=(Xnoisy_val, Yclean_val), epochs=20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(history.history.keys())  # Print available keys\n\n# # Plot training history\n# import matplotlib.pyplot as plt\n\n# # Plot training & validation loss values\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('Model Loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Validation'], loc='upper left')\n# plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.summary()\nsr = 22050","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CLEAN AUDIO FILE EXTRACTION**\n* Predicting the clean audio\n* Saving the audio for futher transciption and summarization","metadata":{}},{"cell_type":"code","source":"from IPython.display import Audio \ntest_path = '/kaggle/input/noisy-audio/Noisy/03-01-01-01-01-01-03_noisy.wav'\n# audio,_ = librosa.load(test_path)\n# pred = model.predict(audio)\n\nXaudio, sr = librosa.load(test_path, sr=sr,duration=duration)  # Load with original sample rate\nXaudio = np.array(Xaudio)\nprint(Xaudio.shape)\nXaudio = Xaudio[..., np.newaxis]\nprint(Xaudio)\nXaudio = Xaudio.reshape(-1, 65536, 1)\n\nprint(Xaudio.shape)\n# audio_input = audio.reshape(1, -1)  \n# (None, 65536, 1)\nprediction = model.predict(Xaudio)\nprint(prediction)\n\noutput_path = '/kaggle/working/clean_audio.wav'\nsf.write(output_path, prediction.squeeze(), sr)\n\nAudio(Xaudio.squeeze(), rate=sr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WHISPER MODEL -- Transcription","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n\nprocessor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3\")\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-large-v3\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_features = processor(\n#     waveform, sampling_rate=sampling_rate, return_tensors=\"pt\"\n# ).input_features\n\n# # Generate token ids\n# predicted_ids = model.generate(input_features)\n\n# # Decode token ids to text\n# transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n\n# transcription[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/openai/whisper","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import whisper\n\nmodel = whisper.load_model(\"base\")\n\naudio = whisper.load_audio(output_path)\naudio = whisper.pad_or_trim(audio)\n\nmel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")\n\noptions = whisper.DecodingOptions()\nresult = whisper.decode(model, mel, options)\n\nprint(result.text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**result.text is my transcribed text**","metadata":{}},{"cell_type":"code","source":"text_transcribed = result.text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summarisation model FLAN-T5","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers\nimport transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer\nfrom transformers import GenerationConfig","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CHECKING MY TRANSCRIPTION MODEL AND TOKENIZER**","metadata":{}},{"cell_type":"code","source":"sentence = \"What time is it, Tom?\"\n\nsentence_encoded = tokenizer(sentence, return_tensors='pt')\n\nsentence_decoded = tokenizer.decode(\n        sentence_encoded[\"input_ids\"][0],\n        skip_special_tokens=True\n    )\n\nprint('ENCODED SENTENCE:')\nprint(sentence_encoded[\"input_ids\"][0])\nprint('\\nDECODED SENTENCE:')\nprint(sentence_decoded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***ONE SHOT INFERENCE OR MULTI INFERNECE WAS NOT HELPING***","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n\nexample_text = \"The quick brown fox jumps over the lazy dog. This sentence is often used to test typewriters and keyboards because it contains every letter of the alphabet.\"\nexample_summary = \"A sentence used for testing typewriters and keyboards.\"\n\nprompt = f\"\"\"\nSummarize the following text.\n\nExample:\nText: {example_text}\nSummary: {example_summary}\n\nText: {text_transcribed}\nSummary:\n    \"\"\"\n\nprint(prompt)\n\ninputs = tokenizer(prompt, return_tensors='pt')\n\noutput = tokenizer.decode(\n  model.generate(\n    inputs[\"input_ids\"],\n    max_new_tokens=50,\n  )[0],\n  skip_special_tokens=True\n)\n\nprint(\"Predicted summary = \", output)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ____END OF PROJECT_____","metadata":{}}]}